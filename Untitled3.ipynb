{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOrPsIO51HEvX/UgTU2cg6M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"v05mtxG01YWJ"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LwlYswe1VBG","executionInfo":{"status":"ok","timestamp":1748854249092,"user_tz":-240,"elapsed":304,"user":{"displayName":"German Fernandez Cantos","userId":"04600956275989479437"}},"outputId":"70fa6735-d385-4b03-a1bd-35a686dc5c3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Libraries imported successfully!\n","Pandas version: 2.2.2\n","NumPy version: 2.0.2\n","\n","==================================================\n","CREATING SAMPLE DATASET FOR DEMONSTRATION\n","==================================================\n","Sample dataset created for demonstration:\n","      Name  Age Department  Salary  Years_Experience      City  \\\n","0    Alice   25         IT   50000                 3  New York   \n","1      Bob   30         HR   45000                 5   Chicago   \n","2  Charlie   35         IT   60000                 8  New York   \n","3    Diana   28    Finance   55000                 4    Boston   \n","4      Eve   33         IT   58000                 6   Chicago   \n","5    Frank   29         HR   47000                 3    Boston   \n","6    Grace   31    Finance   62000                 7  New York   \n","7    Henry   27         IT   52000                 4   Chicago   \n","\n","   Performance_Score  \n","0                8.5  \n","1                7.2  \n","2                9.1  \n","3                8.0  \n","4                8.8  \n","5                7.5  \n","6                9.0  \n","7                8.3  \n","\n","==================================================\n","BASIC DATAFRAME ANALYSIS\n","==================================================\n","üìä DATASET OVERVIEW:\n","--------------------\n","Number of rows: 8\n","Number of columns: 7\n","Total number of values: 56\n","\n","üìã COLUMN INFORMATION:\n","--------------------\n","Column names: ['Name', 'Age', 'Department', 'Salary', 'Years_Experience', 'City', 'Performance_Score']\n","\n","Data types:\n","Name                  object\n","Age                    int64\n","Department            object\n","Salary                 int64\n","Years_Experience       int64\n","City                  object\n","Performance_Score    float64\n","dtype: object\n","\n","üíæ MEMORY USAGE:\n","--------------------\n","Total memory usage: 1,877 bytes (0.00 MB)\n","\n","‚ùì MISSING VALUES ANALYSIS:\n","--------------------\n","              Column  Missing_Count  Missing_Percentage\n","0               Name              0                 0.0\n","1                Age              0                 0.0\n","2         Department              0                 0.0\n","3             Salary              0                 0.0\n","4   Years_Experience              0                 0.0\n","5               City              0                 0.0\n","6  Performance_Score              0                 0.0\n","\n","üìà BASIC STATISTICS:\n","--------------------\n","         Name       Age Department        Salary  Years_Experience      City  \\\n","count       8   8.00000          8      8.000000           8.00000         8   \n","unique      8       NaN          3           NaN               NaN         3   \n","top     Alice       NaN         IT           NaN               NaN  New York   \n","freq        1       NaN          4           NaN               NaN         3   \n","mean      NaN  29.75000        NaN  53625.000000           5.00000       NaN   \n","std       NaN   3.24037        NaN   6162.965427           1.85164       NaN   \n","min       NaN  25.00000        NaN  45000.000000           3.00000       NaN   \n","25%       NaN  27.75000        NaN  49250.000000           3.75000       NaN   \n","50%       NaN  29.50000        NaN  53500.000000           4.50000       NaN   \n","75%       NaN  31.50000        NaN  58500.000000           6.25000       NaN   \n","max       NaN  35.00000        NaN  62000.000000           8.00000       NaN   \n","\n","        Performance_Score  \n","count             8.00000  \n","unique                NaN  \n","top                   NaN  \n","freq                  NaN  \n","mean              8.30000  \n","std               0.69282  \n","min               7.20000  \n","25%               7.87500  \n","50%               8.40000  \n","75%               8.85000  \n","max               9.10000  \n","\n","============================================================\n","DETAILED COLUMN ANALYSIS\n","============================================================\n","\n","üîç ANALYSIS FOR COLUMN: 'Name'\n","----------------------------------------\n","Data type: object\n","Non-null count: 8\n","Null count: 0\n","Unique values: 8\n","\n","üìù CATEGORICAL STATISTICS:\n","  Top 10 most frequent values:\n","    'Alice': 1 (12.5%)\n","    'Bob': 1 (12.5%)\n","    'Charlie': 1 (12.5%)\n","    'Diana': 1 (12.5%)\n","    'Eve': 1 (12.5%)\n","    'Frank': 1 (12.5%)\n","    'Grace': 1 (12.5%)\n","    'Henry': 1 (12.5%)\n","----------------------------------------\n","\n","üîç ANALYSIS FOR COLUMN: 'Age'\n","----------------------------------------\n","Data type: int64\n","Non-null count: 8\n","Null count: 0\n","Unique values: 8\n","\n","üìä NUMERIC STATISTICS:\n","  Min: 25\n","  Max: 35\n","  Mean: 29.75\n","  Median: 29.50\n","  Standard deviation: 3.24\n","  Sum: 238.00\n","  Q1 (25th percentile): 27.75\n","  Q3 (75th percentile): 31.50\n","  IQR: 3.75\n","----------------------------------------\n","\n","üîç ANALYSIS FOR COLUMN: 'Department'\n","----------------------------------------\n","Data type: object\n","Non-null count: 8\n","Null count: 0\n","Unique values: 3\n","\n","üìù CATEGORICAL STATISTICS:\n","  Top 10 most frequent values:\n","    'IT': 4 (50.0%)\n","    'HR': 2 (25.0%)\n","    'Finance': 2 (25.0%)\n","----------------------------------------\n","\n","üîç ANALYSIS FOR COLUMN: 'Salary'\n","----------------------------------------\n","Data type: int64\n","Non-null count: 8\n","Null count: 0\n","Unique values: 8\n","\n","üìä NUMERIC STATISTICS:\n","  Min: 45000\n","  Max: 62000\n","  Mean: 53625.00\n","  Median: 53500.00\n","  Standard deviation: 6162.97\n","  Sum: 429,000.00\n","  Q1 (25th percentile): 49250.00\n","  Q3 (75th percentile): 58500.00\n","  IQR: 9250.00\n","----------------------------------------\n","\n","üîç ANALYSIS FOR COLUMN: 'Years_Experience'\n","----------------------------------------\n","Data type: int64\n","Non-null count: 8\n","Null count: 0\n","Unique values: 6\n","\n","üìä NUMERIC STATISTICS:\n","  Min: 3\n","  Max: 8\n","  Mean: 5.00\n","  Median: 4.50\n","  Standard deviation: 1.85\n","  Sum: 40.00\n","  Q1 (25th percentile): 3.75\n","  Q3 (75th percentile): 6.25\n","  IQR: 2.50\n","----------------------------------------\n","\n","üîç ANALYSIS FOR COLUMN: 'City'\n","----------------------------------------\n","Data type: object\n","Non-null count: 8\n","Null count: 0\n","Unique values: 3\n","\n","üìù CATEGORICAL STATISTICS:\n","  Top 10 most frequent values:\n","    'New York': 3 (37.5%)\n","    'Chicago': 3 (37.5%)\n","    'Boston': 2 (25.0%)\n","----------------------------------------\n","\n","üîç ANALYSIS FOR COLUMN: 'Performance_Score'\n","----------------------------------------\n","Data type: float64\n","Non-null count: 8\n","Null count: 0\n","Unique values: 8\n","\n","üìä NUMERIC STATISTICS:\n","  Min: 7.2\n","  Max: 9.1\n","  Mean: 8.30\n","  Median: 8.40\n","  Standard deviation: 0.69\n","  Sum: 66.40\n","  Q1 (25th percentile): 7.88\n","  Q3 (75th percentile): 8.85\n","  IQR: 0.98\n","----------------------------------------\n","\n","==================================================\n","AGGREGATION ANALYSIS\n","==================================================\n","Numeric columns: ['Age', 'Salary', 'Years_Experience', 'Performance_Score']\n","Categorical columns: ['Name', 'Department', 'City']\n","\n","üìà NUMERIC AGGREGATIONS:\n","------------------------------\n","Overall numeric aggregations:\n","           Age     Salary  Years_Experience  Performance_Score\n","count     8.00       8.00              8.00               8.00\n","sum     238.00  429000.00             40.00              66.40\n","mean     29.75   53625.00              5.00               8.30\n","median   29.50   53500.00              4.50               8.40\n","min      25.00   45000.00              3.00               7.20\n","max      35.00   62000.00              8.00               9.10\n","std       3.24    6162.97              1.85               0.69\n","\n","üéØ CUSTOM AGGREGATIONS:\n","-------------------------\n","\n","Column: Age\n","  Total sum: 238.00\n","  Average: 29.75\n","  Range: 10.00\n","  Coefficient of variation: 10.9%\n","\n","Column: Salary\n","  Total sum: 429,000.00\n","  Average: 53625.00\n","  Range: 17000.00\n","  Coefficient of variation: 11.5%\n","\n","Column: Years_Experience\n","  Total sum: 40.00\n","  Average: 5.00\n","  Range: 5.00\n","  Coefficient of variation: 37.0%\n","\n","Column: Performance_Score\n","  Total sum: 66.40\n","  Average: 8.30\n","  Range: 1.90\n","  Coefficient of variation: 8.3%\n","\n","üè∑Ô∏è GROUP-BASED AGGREGATIONS:\n","------------------------------\n","\n","Grouped by 'Name':\n","          Age                   Salary                                \\\n","        count  mean sum min max  count     mean    sum    min    max   \n","Name                                                                   \n","Alice       1  25.0  25  25  25      1  50000.0  50000  50000  50000   \n","Bob         1  30.0  30  30  30      1  45000.0  45000  45000  45000   \n","Charlie     1  35.0  35  35  35      1  60000.0  60000  60000  60000   \n","Diana       1  28.0  28  28  28      1  55000.0  55000  55000  55000   \n","Eve         1  33.0  33  33  33      1  58000.0  58000  58000  58000   \n","Frank       1  29.0  29  29  29      1  47000.0  47000  47000  47000   \n","Grace       1  31.0  31  31  31      1  62000.0  62000  62000  62000   \n","Henry       1  27.0  27  27  27      1  52000.0  52000  52000  52000   \n","\n","        Years_Experience                  Performance_Score                 \\\n","                   count mean sum min max             count mean  sum  min   \n","Name                                                                         \n","Alice                  1  3.0   3   3   3                 1  8.5  8.5  8.5   \n","Bob                    1  5.0   5   5   5                 1  7.2  7.2  7.2   \n","Charlie                1  8.0   8   8   8                 1  9.1  9.1  9.1   \n","Diana                  1  4.0   4   4   4                 1  8.0  8.0  8.0   \n","Eve                    1  6.0   6   6   6                 1  8.8  8.8  8.8   \n","Frank                  1  3.0   3   3   3                 1  7.5  7.5  7.5   \n","Grace                  1  7.0   7   7   7                 1  9.0  9.0  9.0   \n","Henry                  1  4.0   4   4   4                 1  8.3  8.3  8.3   \n","\n","              \n","         max  \n","Name          \n","Alice    8.5  \n","Bob      7.2  \n","Charlie  9.1  \n","Diana    8.0  \n","Eve      8.8  \n","Frank    7.5  \n","Grace    9.0  \n","Henry    8.3  \n","\n","Grouped by 'Department':\n","             Age                    Salary                                 \\\n","           count  mean  sum min max  count     mean     sum    min    max   \n","Department                                                                  \n","Finance        2  29.5   59  28  31      2  58500.0  117000  55000  62000   \n","HR             2  29.5   59  29  30      2  46000.0   92000  45000  47000   \n","IT             4  30.0  120  25  35      4  55000.0  220000  50000  60000   \n","\n","           Years_Experience                   Performance_Score              \\\n","                      count  mean sum min max             count  mean   sum   \n","Department                                                                    \n","Finance                   2  5.50  11   4   7                 2  8.50  17.0   \n","HR                        2  4.00   8   3   5                 2  7.35  14.7   \n","IT                        4  5.25  21   3   8                 4  8.68  34.7   \n","\n","                      \n","            min  max  \n","Department            \n","Finance     8.0  9.0  \n","HR          7.2  7.5  \n","IT          8.3  9.1  \n","\n","============================================================\n","CREATING NEW TABLES - COLUMN OPERATIONS\n","============================================================\n","‚úÖ Created table with only numeric columns: ['Age', 'Salary', 'Years_Experience', 'Performance_Score']\n","Shape: (8, 4)\n","   Age  Salary  Years_Experience  Performance_Score\n","0   25   50000                 3                8.5\n","1   30   45000                 5                7.2\n","2   35   60000                 8                9.1\n","3   28   55000                 4                8.0\n","4   33   58000                 6                8.8\n","\n","‚úÖ Created table with only categorical columns: ['Name', 'Department', 'City']\n","Shape: (8, 3)\n","      Name Department      City\n","0    Alice         IT  New York\n","1      Bob         HR   Chicago\n","2  Charlie         IT  New York\n","3    Diana    Finance    Boston\n","4      Eve         IT   Chicago\n","\n","‚úÖ Created table without columns ['Name']\n","Remaining columns: ['Age', 'Department', 'Salary', 'Years_Experience', 'City', 'Performance_Score']\n","Shape: (8, 6)\n","   Age Department  Salary  Years_Experience      City  Performance_Score\n","0   25         IT   50000                 3  New York                8.5\n","1   30         HR   45000                 5   Chicago                7.2\n","2   35         IT   60000                 8  New York                9.1\n","3   28    Finance   55000                 4    Boston                8.0\n","4   33         IT   58000                 6   Chicago                8.8\n","\n","‚úÖ Created table with columns containing 'e': ['Name', 'Age', 'Department', 'Years_Experience', 'Performance_Score']\n","Shape: (8, 5)\n","      Name  Age Department  Years_Experience  Performance_Score\n","0    Alice   25         IT                 3                8.5\n","1      Bob   30         HR                 5                7.2\n","2  Charlie   35         IT                 8                9.1\n","3    Diana   28    Finance                 4                8.0\n","4      Eve   33         IT                 6                8.8\n","\n","‚úÖ Created table with first 3 columns: ['Name', 'Age', 'Department']\n","Shape: (8, 3)\n","      Name  Age Department\n","0    Alice   25         IT\n","1      Bob   30         HR\n","2  Charlie   35         IT\n","3    Diana   28    Finance\n","4      Eve   33         IT\n","\n","============================================================\n","CREATING NEW TABLES - ROW OPERATIONS\n","============================================================\n","‚úÖ Created table with Age > 30\n","Original rows: 8, Filtered rows: 3\n","      Name  Age Department  Salary  Years_Experience      City  \\\n","2  Charlie   35         IT   60000                 8  New York   \n","4      Eve   33         IT   58000                 6   Chicago   \n","6    Grace   31    Finance   62000                 7  New York   \n","\n","   Performance_Score  \n","2                9.1  \n","4                8.8  \n","6                9.0  \n","\n","‚úÖ Created table with 5 random rows\n","      Name  Age Department  Salary  Years_Experience      City  \\\n","1      Bob   30         HR   45000                 5   Chicago   \n","5    Frank   29         HR   47000                 3    Boston   \n","0    Alice   25         IT   50000                 3  New York   \n","7    Henry   27         IT   52000                 4   Chicago   \n","2  Charlie   35         IT   60000                 8  New York   \n","\n","   Performance_Score  \n","1                7.2  \n","5                7.5  \n","0                8.5  \n","7                8.3  \n","2                9.1  \n","\n","‚úÖ Created table with top 3 rows\n","      Name  Age Department  Salary  Years_Experience      City  \\\n","0    Alice   25         IT   50000                 3  New York   \n","1      Bob   30         HR   45000                 5   Chicago   \n","2  Charlie   35         IT   60000                 8  New York   \n","\n","   Performance_Score  \n","0                8.5  \n","1                7.2  \n","2                9.1  \n","\n","‚úÖ Created table with bottom 3 rows\n","    Name  Age Department  Salary  Years_Experience      City  \\\n","5  Frank   29         HR   47000                 3    Boston   \n","6  Grace   31    Finance   62000                 7  New York   \n","7  Henry   27         IT   52000                 4   Chicago   \n","\n","   Performance_Score  \n","5                7.5  \n","6                9.0  \n","7                8.3  \n","\n","‚úÖ Created table without missing values\n","Original rows: 8, After removing missing: 8\n","\n","‚úÖ Created table with IT employees earning > 50000\n","Filtered rows: 3\n","      Name  Age Department  Salary  Years_Experience      City  \\\n","2  Charlie   35         IT   60000                 8  New York   \n","4      Eve   33         IT   58000                 6   Chicago   \n","7    Henry   27         IT   52000                 4   Chicago   \n","\n","   Performance_Score  \n","2                9.1  \n","4                8.8  \n","7                8.3  \n","\n","‚úÖ Created table without duplicate rows\n","Original rows: 8, After removing duplicates: 8\n","\n","============================================================\n","ADVANCED DATA MANIPULATION\n","============================================================\n","üìä PIVOT TABLE ANALYSIS:\n","-------------------------\n","Salary analysis by Department:\n","               mean     sum  count\n","             Salary  Salary Salary\n","Department                        \n","Finance     58500.0  117000      2\n","HR          46000.0   92000      2\n","IT          55000.0  220000      4\n","\n","üîÑ CROSS-TABULATION:\n","--------------------\n","Department vs City cross-tabulation:\n","City        Boston  Chicago  New York  All\n","Department                                \n","Finance          1        0         1    2\n","HR               1        1         0    2\n","IT               0        2         2    4\n","All              2        3         3    8\n","\n","üîó CORRELATION ANALYSIS:\n","-----------------------\n","Correlation matrix:\n","                     Age  Salary  Years_Experience  Performance_Score\n","Age                1.000   0.574             0.881              0.420\n","Salary             0.574   1.000             0.764              0.897\n","Years_Experience   0.881   0.764             1.000              0.646\n","Performance_Score  0.420   0.897             0.646              1.000\n","\n","üèÜ RANKING ANALYSIS:\n","------------------\n","Top employees by salary:\n","      Name  Salary  Salary_Rank  Salary_Percentile\n","0    Alice   50000          6.0               37.5\n","1      Bob   45000          8.0               12.5\n","2  Charlie   60000          2.0               87.5\n","3    Diana   55000          4.0               62.5\n","4      Eve   58000          3.0               75.0\n","\n","============================================================\n","COMPREHENSIVE SUMMARY REPORT\n","============================================================\n","üìã DATASET SUMMARY:\n","------------------\n","‚Ä¢ Dataset shape: 8 rows √ó 7 columns\n","‚Ä¢ Total data points: 56\n","‚Ä¢ Memory usage: 1,877 bytes\n","‚Ä¢ Missing values: 0 (0.0%)\n","\n","üìä DATA TYPES BREAKDOWN:\n","-------------------------\n","‚Ä¢ object: 3 columns\n","‚Ä¢ int64: 3 columns\n","‚Ä¢ float64: 1 columns\n","\n","üí∞ NUMERIC COLUMNS SUMMARY:\n","---------------------------\n","‚Ä¢ Age:\n","  Sum: 238.00\n","  Average: 29.75\n","  Range: 25.00 to 35.00\n","‚Ä¢ Salary:\n","  Sum: 429,000.00\n","  Average: 53625.00\n","  Range: 45000.00 to 62000.00\n","‚Ä¢ Years_Experience:\n","  Sum: 40.00\n","  Average: 5.00\n","  Range: 3.00 to 8.00\n","‚Ä¢ Performance_Score:\n","  Sum: 66.40\n","  Average: 8.30\n","  Range: 7.20 to 9.10\n","\n","üè∑Ô∏è CATEGORICAL COLUMNS SUMMARY:\n","-------------------------------\n","‚Ä¢ Name: 8 unique values, most common: 'Alice'\n","‚Ä¢ Department: 3 unique values, most common: 'IT'\n","‚Ä¢ City: 3 unique values, most common: 'Chicago'\n","\n","‚úÖ Analysis completed successfully!\n","üöÄ You can now use the processed tables for further analysis or export them.\n","\n","============================================================\n","QUICK REFERENCE - COMMON OPERATIONS\n","============================================================\n","\n","üîß COMMON CSV OPERATIONS CHEAT SHEET:\n","\n","1. LOADING DATA:\n","   df = pd.read_csv('file.csv')\n","   df = pd.read_csv('file.csv', encoding='latin1')  # For special characters\n","   df = pd.read_csv('file.csv', sep=';')            # Different separator\n","\n","2. BASIC INSPECTION:\n","   df.shape                    # Dimensions\n","   df.info()                   # Data types and non-null counts\n","   df.describe()               # Statistical summary\n","   df.head(n)                  # First n rows\n","   df.tail(n)                  # Last n rows\n","\n","3. COLUMN OPERATIONS:\n","   df.columns                  # Column names\n","   df.dtypes                   # Data types\n","   df[['col1', 'col2']]       # Select columns\n","   df.drop(['col1'], axis=1)   # Remove columns\n","\n","4. ROW OPERATIONS:\n","   df[df['col'] > value]       # Filter rows\n","   df.sample(n=10)            # Random sample\n","   df.drop_duplicates()        # Remove duplicates\n","   df.dropna()                # Remove missing values\n","\n","5. AGGREGATIONS:\n","   df['col'].sum()            # Sum\n","   df['col'].mean()           # Average\n","   df.groupby('col').sum()    # Group by aggregation\n","   df.pivot_table()           # Pivot table\n","\n","6. EXPORTS:\n","   df.to_csv('output.csv', index=False)\n","   df.to_excel('output.xlsx', index=False)\n","   df.to_json('output.json', orient='records')\n","\n","\n","üéâ CSV Analysis Tutorial Complete!\n","üí° Pro tip: Always start with df.info() and df.describe() to understand your data!\n"]}],"source":["# CSV Analysis in Jupyter Notebook - Complete Step-by-Step Guide\n","# ==============================================================\n","\n","# Step 1: Import Required Libraries\n","# ---------------------------------\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set display options for better output\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', 50)\n","\n","print(\"Libraries imported successfully!\")\n","print(\"Pandas version:\", pd.__version__)\n","print(\"NumPy version:\", np.__version__)\n","\n","# Step 2: Load and Inspect the CSV File\n","# ------------------------------------\n","\n","# Method 1: Load CSV with basic parameters\n","def load_csv_file(file_path, encoding='utf-8', separator=','):\n","    \"\"\"\n","    Load CSV file with error handling and basic inspection\n","\n","    Parameters:\n","    file_path (str): Path to the CSV file\n","    encoding (str): File encoding (default: 'utf-8')\n","    separator (str): Column separator (default: ',')\n","\n","    Returns:\n","    pandas.DataFrame: Loaded data\n","    \"\"\"\n","    try:\n","        # Load the CSV file\n","        df = pd.read_csv(file_path, encoding=encoding, sep=separator)\n","        print(f\"‚úÖ CSV file loaded successfully!\")\n","        print(f\"File path: {file_path}\")\n","        return df\n","    except FileNotFoundError:\n","        print(f\"‚ùå Error: File '{file_path}' not found.\")\n","        return None\n","    except pd.errors.EmptyDataError:\n","        print(\"‚ùå Error: The CSV file is empty.\")\n","        return None\n","    except Exception as e:\n","        print(f\"‚ùå Error loading CSV: {str(e)}\")\n","        return None\n","\n","# Example usage (replace with your actual file path):\n","# df = load_csv_file('your_file.csv')\n","\n","# For demonstration, let's create a sample dataset\n","print(\"\\n\" + \"=\"*50)\n","print(\"CREATING SAMPLE DATASET FOR DEMONSTRATION\")\n","print(\"=\"*50)\n","\n","# Create sample data\n","np.random.seed(42)\n","sample_data = {\n","    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve', 'Frank', 'Grace', 'Henry'],\n","    'Age': [25, 30, 35, 28, 33, 29, 31, 27],\n","    'Department': ['IT', 'HR', 'IT', 'Finance', 'IT', 'HR', 'Finance', 'IT'],\n","    'Salary': [50000, 45000, 60000, 55000, 58000, 47000, 62000, 52000],\n","    'Years_Experience': [3, 5, 8, 4, 6, 3, 7, 4],\n","    'City': ['New York', 'Chicago', 'New York', 'Boston', 'Chicago', 'Boston', 'New York', 'Chicago'],\n","    'Performance_Score': [8.5, 7.2, 9.1, 8.0, 8.8, 7.5, 9.0, 8.3]\n","}\n","\n","df = pd.DataFrame(sample_data)\n","print(\"Sample dataset created for demonstration:\")\n","print(df)\n","\n","# Step 3: Basic Data Inspection and Analysis\n","# ------------------------------------------\n","\n","def analyze_dataframe_basics(df):\n","    \"\"\"\n","    Perform basic analysis of the dataframe\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input dataframe\n","\n","    Returns:\n","    dict: Dictionary containing basic statistics\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"BASIC DATAFRAME ANALYSIS\")\n","    print(\"=\"*50)\n","\n","    # Basic information\n","    print(\"üìä DATASET OVERVIEW:\")\n","    print(\"-\" * 20)\n","\n","    # Number of rows and columns\n","    num_rows, num_cols = df.shape\n","    print(f\"Number of rows: {num_rows:,}\")\n","    print(f\"Number of columns: {num_cols}\")\n","    print(f\"Total number of values: {num_rows * num_cols:,}\")\n","\n","    # Column information\n","    print(f\"\\nüìã COLUMN INFORMATION:\")\n","    print(\"-\" * 20)\n","    print(\"Column names:\", list(df.columns))\n","    print(\"\\nData types:\")\n","    print(df.dtypes)\n","\n","    # Memory usage\n","    print(f\"\\nüíæ MEMORY USAGE:\")\n","    print(\"-\" * 20)\n","    memory_usage = df.memory_usage(deep=True).sum()\n","    print(f\"Total memory usage: {memory_usage:,} bytes ({memory_usage/1024/1024:.2f} MB)\")\n","\n","    # Missing values analysis\n","    print(f\"\\n‚ùì MISSING VALUES ANALYSIS:\")\n","    print(\"-\" * 20)\n","    missing_values = df.isnull().sum()\n","    missing_percentage = (missing_values / len(df)) * 100\n","\n","    missing_summary = pd.DataFrame({\n","        'Column': df.columns,\n","        'Missing_Count': missing_values.values,\n","        'Missing_Percentage': missing_percentage.values\n","    })\n","    print(missing_summary)\n","\n","    # Basic statistics\n","    print(f\"\\nüìà BASIC STATISTICS:\")\n","    print(\"-\" * 20)\n","    print(df.describe(include='all'))\n","\n","    return {\n","        'num_rows': num_rows,\n","        'num_cols': num_cols,\n","        'total_values': num_rows * num_cols,\n","        'columns': list(df.columns),\n","        'dtypes': df.dtypes.to_dict(),\n","        'missing_values': missing_values.to_dict(),\n","        'memory_usage_bytes': memory_usage\n","    }\n","\n","# Run basic analysis\n","basic_stats = analyze_dataframe_basics(df)\n","\n","# Step 4: Advanced Column Analysis\n","# --------------------------------\n","\n","def analyze_columns_detailed(df):\n","    \"\"\"\n","    Perform detailed analysis of each column\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input dataframe\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"DETAILED COLUMN ANALYSIS\")\n","    print(\"=\"*60)\n","\n","    for column in df.columns:\n","        print(f\"\\nüîç ANALYSIS FOR COLUMN: '{column}'\")\n","        print(\"-\" * 40)\n","\n","        col_data = df[column]\n","\n","        # Basic info\n","        print(f\"Data type: {col_data.dtype}\")\n","        print(f\"Non-null count: {col_data.count():,}\")\n","        print(f\"Null count: {col_data.isnull().sum():,}\")\n","        print(f\"Unique values: {col_data.nunique():,}\")\n","\n","        # For numeric columns\n","        if pd.api.types.is_numeric_dtype(col_data):\n","            print(f\"\\nüìä NUMERIC STATISTICS:\")\n","            print(f\"  Min: {col_data.min()}\")\n","            print(f\"  Max: {col_data.max()}\")\n","            print(f\"  Mean: {col_data.mean():.2f}\")\n","            print(f\"  Median: {col_data.median():.2f}\")\n","            print(f\"  Standard deviation: {col_data.std():.2f}\")\n","            print(f\"  Sum: {col_data.sum():,.2f}\")\n","\n","            # Quartiles\n","            q1 = col_data.quantile(0.25)\n","            q3 = col_data.quantile(0.75)\n","            print(f\"  Q1 (25th percentile): {q1:.2f}\")\n","            print(f\"  Q3 (75th percentile): {q3:.2f}\")\n","            print(f\"  IQR: {q3 - q1:.2f}\")\n","\n","        # For categorical/text columns\n","        else:\n","            print(f\"\\nüìù CATEGORICAL STATISTICS:\")\n","            value_counts = col_data.value_counts().head(10)\n","            print(\"  Top 10 most frequent values:\")\n","            for value, count in value_counts.items():\n","                percentage = (count / len(col_data)) * 100\n","                print(f\"    '{value}': {count} ({percentage:.1f}%)\")\n","\n","        print(\"-\" * 40)\n","\n","# Run detailed column analysis\n","analyze_columns_detailed(df)\n","\n","# Step 5: Aggregation Functions\n","# -----------------------------\n","\n","def perform_aggregations(df):\n","    \"\"\"\n","    Perform various aggregation operations on the dataframe\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input dataframe\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"AGGREGATION ANALYSIS\")\n","    print(\"=\"*50)\n","\n","    # Get numeric columns only\n","    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n","    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n","\n","    print(f\"Numeric columns: {numeric_columns}\")\n","    print(f\"Categorical columns: {categorical_columns}\")\n","\n","    if numeric_columns:\n","        print(f\"\\nüìà NUMERIC AGGREGATIONS:\")\n","        print(\"-\" * 30)\n","\n","        # Overall aggregations for all numeric columns\n","        agg_results = df[numeric_columns].agg(['count', 'sum', 'mean', 'median', 'min', 'max', 'std'])\n","        print(\"Overall numeric aggregations:\")\n","        print(agg_results.round(2))\n","\n","        # Custom aggregations\n","        print(f\"\\nüéØ CUSTOM AGGREGATIONS:\")\n","        print(\"-\" * 25)\n","\n","        for col in numeric_columns:\n","            print(f\"\\nColumn: {col}\")\n","            print(f\"  Total sum: {df[col].sum():,.2f}\")\n","            print(f\"  Average: {df[col].mean():.2f}\")\n","            print(f\"  Range: {df[col].max() - df[col].min():.2f}\")\n","            print(f\"  Coefficient of variation: {(df[col].std() / df[col].mean() * 100):.1f}%\")\n","\n","    # Group-based aggregations\n","    if categorical_columns and numeric_columns:\n","        print(f\"\\nüè∑Ô∏è GROUP-BASED AGGREGATIONS:\")\n","        print(\"-\" * 30)\n","\n","        for cat_col in categorical_columns[:2]:  # Limit to first 2 categorical columns\n","            print(f\"\\nGrouped by '{cat_col}':\")\n","            grouped = df.groupby(cat_col)[numeric_columns].agg(['count', 'mean', 'sum', 'min', 'max'])\n","            print(grouped.round(2))\n","\n","# Run aggregation analysis\n","perform_aggregations(df)\n","\n","# Step 6: Creating New Tables - Missing Columns\n","# ---------------------------------------------\n","\n","def create_tables_missing_columns(df):\n","    \"\"\"\n","    Create new tables by removing or selecting specific columns\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input dataframe\n","\n","    Returns:\n","    dict: Dictionary containing various filtered dataframes\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"CREATING NEW TABLES - COLUMN OPERATIONS\")\n","    print(\"=\"*60)\n","\n","    tables = {}\n","\n","    # 1. Select only numeric columns\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n","    if numeric_cols:\n","        tables['numeric_only'] = df[numeric_cols].copy()\n","        print(f\"‚úÖ Created table with only numeric columns: {numeric_cols}\")\n","        print(f\"Shape: {tables['numeric_only'].shape}\")\n","        print(tables['numeric_only'].head())\n","\n","    # 2. Select only categorical columns\n","    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n","    if categorical_cols:\n","        tables['categorical_only'] = df[categorical_cols].copy()\n","        print(f\"\\n‚úÖ Created table with only categorical columns: {categorical_cols}\")\n","        print(f\"Shape: {tables['categorical_only'].shape}\")\n","        print(tables['categorical_only'].head())\n","\n","    # 3. Remove specific columns (example: remove 'Name' if exists)\n","    columns_to_remove = ['Name']  # Customize this list\n","    remaining_cols = [col for col in df.columns if col not in columns_to_remove]\n","    if len(remaining_cols) < len(df.columns):\n","        tables['without_specified_cols'] = df[remaining_cols].copy()\n","        print(f\"\\n‚úÖ Created table without columns {columns_to_remove}\")\n","        print(f\"Remaining columns: {remaining_cols}\")\n","        print(f\"Shape: {tables['without_specified_cols'].shape}\")\n","        print(tables['without_specified_cols'].head())\n","\n","    # 4. Select columns by pattern (example: columns containing 'e')\n","    pattern_cols = [col for col in df.columns if 'e' in col.lower()]\n","    if pattern_cols:\n","        tables['pattern_match'] = df[pattern_cols].copy()\n","        print(f\"\\n‚úÖ Created table with columns containing 'e': {pattern_cols}\")\n","        print(f\"Shape: {tables['pattern_match'].shape}\")\n","        print(tables['pattern_match'].head())\n","\n","    # 5. Select first N columns\n","    n_cols = 3\n","    if len(df.columns) > n_cols:\n","        tables['first_n_columns'] = df.iloc[:, :n_cols].copy()\n","        print(f\"\\n‚úÖ Created table with first {n_cols} columns: {list(tables['first_n_columns'].columns)}\")\n","        print(f\"Shape: {tables['first_n_columns'].shape}\")\n","        print(tables['first_n_columns'].head())\n","\n","    return tables\n","\n","# Create tables with missing columns\n","column_tables = create_tables_missing_columns(df)\n","\n","# Step 7: Creating New Tables - Missing Rows\n","# ------------------------------------------\n","\n","def create_tables_missing_rows(df):\n","    \"\"\"\n","    Create new tables by removing or selecting specific rows\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input dataframe\n","\n","    Returns:\n","    dict: Dictionary containing various filtered dataframes\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"CREATING NEW TABLES - ROW OPERATIONS\")\n","    print(\"=\"*60)\n","\n","    tables = {}\n","\n","    # 1. Filter rows based on conditions\n","    if 'Age' in df.columns:\n","        tables['age_filter'] = df[df['Age'] > 30].copy()\n","        print(f\"‚úÖ Created table with Age > 30\")\n","        print(f\"Original rows: {len(df)}, Filtered rows: {len(tables['age_filter'])}\")\n","        print(tables['age_filter'])\n","\n","    # 2. Sample random rows\n","    sample_size = min(5, len(df))\n","    tables['random_sample'] = df.sample(n=sample_size, random_state=42).copy()\n","    print(f\"\\n‚úÖ Created table with {sample_size} random rows\")\n","    print(tables['random_sample'])\n","\n","    # 3. Get top N rows\n","    n_rows = 3\n","    tables['top_n_rows'] = df.head(n_rows).copy()\n","    print(f\"\\n‚úÖ Created table with top {n_rows} rows\")\n","    print(tables['top_n_rows'])\n","\n","    # 4. Get bottom N rows\n","    tables['bottom_n_rows'] = df.tail(n_rows).copy()\n","    print(f\"\\n‚úÖ Created table with bottom {n_rows} rows\")\n","    print(tables['bottom_n_rows'])\n","\n","    # 5. Remove rows with missing values (if any)\n","    tables['no_missing'] = df.dropna().copy()\n","    print(f\"\\n‚úÖ Created table without missing values\")\n","    print(f\"Original rows: {len(df)}, After removing missing: {len(tables['no_missing'])}\")\n","\n","    # 6. Filter by multiple conditions\n","    if 'Department' in df.columns and 'Salary' in df.columns:\n","        tables['complex_filter'] = df[\n","            (df['Department'] == 'IT') & (df['Salary'] > 50000)\n","        ].copy()\n","        print(f\"\\n‚úÖ Created table with IT employees earning > 50000\")\n","        print(f\"Filtered rows: {len(tables['complex_filter'])}\")\n","        if not tables['complex_filter'].empty:\n","            print(tables['complex_filter'])\n","\n","    # 7. Remove duplicate rows\n","    tables['no_duplicates'] = df.drop_duplicates().copy()\n","    print(f\"\\n‚úÖ Created table without duplicate rows\")\n","    print(f\"Original rows: {len(df)}, After removing duplicates: {len(tables['no_duplicates'])}\")\n","\n","    return tables\n","\n","# Create tables with missing rows\n","row_tables = create_tables_missing_rows(df)\n","\n","# Step 8: Advanced Data Manipulation\n","# ----------------------------------\n","\n","def advanced_data_operations(df):\n","    \"\"\"\n","    Perform advanced data manipulation operations\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input dataframe\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"ADVANCED DATA MANIPULATION\")\n","    print(\"=\"*60)\n","\n","    # 1. Pivot table creation\n","    if 'Department' in df.columns and 'Salary' in df.columns:\n","        print(\"üìä PIVOT TABLE ANALYSIS:\")\n","        print(\"-\" * 25)\n","        pivot_table = df.pivot_table(\n","            values='Salary',\n","            index='Department',\n","            aggfunc=['mean', 'sum', 'count']\n","        )\n","        print(\"Salary analysis by Department:\")\n","        print(pivot_table)\n","\n","    # 2. Cross-tabulation\n","    if 'Department' in df.columns and 'City' in df.columns:\n","        print(\"\\nüîÑ CROSS-TABULATION:\")\n","        print(\"-\" * 20)\n","        crosstab = pd.crosstab(df['Department'], df['City'], margins=True)\n","        print(\"Department vs City cross-tabulation:\")\n","        print(crosstab)\n","\n","    # 3. Correlation analysis\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    if len(numeric_cols) > 1:\n","        print(\"\\nüîó CORRELATION ANALYSIS:\")\n","        print(\"-\" * 23)\n","        correlation_matrix = df[numeric_cols].corr()\n","        print(\"Correlation matrix:\")\n","        print(correlation_matrix.round(3))\n","\n","    # 4. Ranking and percentiles\n","    if 'Salary' in df.columns:\n","        print(\"\\nüèÜ RANKING ANALYSIS:\")\n","        print(\"-\" * 18)\n","        df_ranked = df.copy()\n","        df_ranked['Salary_Rank'] = df_ranked['Salary'].rank(ascending=False)\n","        df_ranked['Salary_Percentile'] = df_ranked['Salary'].rank(pct=True) * 100\n","        print(\"Top employees by salary:\")\n","        print(df_ranked[['Name', 'Salary', 'Salary_Rank', 'Salary_Percentile']].head())\n","\n","# Run advanced operations\n","advanced_data_operations(df)\n","\n","# Step 9: Data Export Functions\n","# -----------------------------\n","\n","def export_data_tables(tables_dict, base_filename=\"processed_data\"):\n","    \"\"\"\n","    Export processed tables to different formats\n","\n","    Parameters:\n","    tables_dict (dict): Dictionary containing dataframes to export\n","    base_filename (str): Base name for exported files\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"DATA EXPORT OPTIONS\")\n","    print(\"=\"*50)\n","\n","    for table_name, table_df in tables_dict.items():\n","        if not table_df.empty:\n","            filename = f\"{base_filename}_{table_name}\"\n","\n","            # Export to CSV\n","            csv_file = f\"{filename}.csv\"\n","            table_df.to_csv(csv_file, index=False)\n","            print(f\"‚úÖ Exported '{table_name}' to {csv_file}\")\n","\n","            # Export to Excel (optional)\n","            try:\n","                excel_file = f\"{filename}.xlsx\"\n","                table_df.to_excel(excel_file, index=False)\n","                print(f\"‚úÖ Exported '{table_name}' to {excel_file}\")\n","            except ImportError:\n","                print(f\"‚ö†Ô∏è  Excel export requires openpyxl: pip install openpyxl\")\n","\n","            # Export to JSON\n","            json_file = f\"{filename}.json\"\n","            table_df.to_json(json_file, orient='records', indent=2)\n","            print(f\"‚úÖ Exported '{table_name}' to {json_file}\")\n","\n","            print(f\"   Shape: {table_df.shape}\")\n","            print()\n","\n","# Example export (uncomment to use):\n","# all_tables = {**column_tables, **row_tables}\n","# export_data_tables(all_tables)\n","\n","# Step 10: Summary Report Generation\n","# ---------------------------------\n","\n","def generate_summary_report(df, basic_stats):\n","    \"\"\"\n","    Generate a comprehensive summary report\n","\n","    Parameters:\n","    df (pandas.DataFrame): Input dataframe\n","    basic_stats (dict): Basic statistics dictionary\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"COMPREHENSIVE SUMMARY REPORT\")\n","    print(\"=\"*60)\n","\n","    print(\"üìã DATASET SUMMARY:\")\n","    print(\"-\" * 18)\n","    print(f\"‚Ä¢ Dataset shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n","    print(f\"‚Ä¢ Total data points: {basic_stats['total_values']:,}\")\n","    print(f\"‚Ä¢ Memory usage: {basic_stats['memory_usage_bytes']:,} bytes\")\n","\n","    # Missing data summary\n","    total_missing = sum(basic_stats['missing_values'].values())\n","    missing_percentage = (total_missing / basic_stats['total_values']) * 100\n","    print(f\"‚Ä¢ Missing values: {total_missing:,} ({missing_percentage:.1f}%)\")\n","\n","    # Data types summary\n","    print(f\"\\nüìä DATA TYPES BREAKDOWN:\")\n","    print(\"-\" * 25)\n","    dtype_counts = pd.Series(basic_stats['dtypes']).value_counts()\n","    for dtype, count in dtype_counts.items():\n","        print(f\"‚Ä¢ {dtype}: {count} columns\")\n","\n","    # Numeric columns summary\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","    if len(numeric_cols) > 0:\n","        print(f\"\\nüí∞ NUMERIC COLUMNS SUMMARY:\")\n","        print(\"-\" * 27)\n","        for col in numeric_cols:\n","            col_sum = df[col].sum()\n","            col_mean = df[col].mean()\n","            print(f\"‚Ä¢ {col}:\")\n","            print(f\"  Sum: {col_sum:,.2f}\")\n","            print(f\"  Average: {col_mean:.2f}\")\n","            print(f\"  Range: {df[col].min():.2f} to {df[col].max():.2f}\")\n","\n","    # Categorical columns summary\n","    cat_cols = df.select_dtypes(include=['object']).columns\n","    if len(cat_cols) > 0:\n","        print(f\"\\nüè∑Ô∏è CATEGORICAL COLUMNS SUMMARY:\")\n","        print(\"-\" * 31)\n","        for col in cat_cols:\n","            unique_count = df[col].nunique()\n","            most_common = df[col].mode().iloc[0] if not df[col].empty else \"N/A\"\n","            print(f\"‚Ä¢ {col}: {unique_count} unique values, most common: '{most_common}'\")\n","\n","    print(f\"\\n‚úÖ Analysis completed successfully!\")\n","    print(f\"üöÄ You can now use the processed tables for further analysis or export them.\")\n","\n","# Generate final summary report\n","generate_summary_report(df, basic_stats)\n","\n","# Step 11: Quick Reference - Common Operations\n","# -------------------------------------------\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"QUICK REFERENCE - COMMON OPERATIONS\")\n","print(\"=\"*60)\n","\n","quick_ref = \"\"\"\n","üîß COMMON CSV OPERATIONS CHEAT SHEET:\n","\n","1. LOADING DATA:\n","   df = pd.read_csv('file.csv')\n","   df = pd.read_csv('file.csv', encoding='latin1')  # For special characters\n","   df = pd.read_csv('file.csv', sep=';')            # Different separator\n","\n","2. BASIC INSPECTION:\n","   df.shape                    # Dimensions\n","   df.info()                   # Data types and non-null counts\n","   df.describe()               # Statistical summary\n","   df.head(n)                  # First n rows\n","   df.tail(n)                  # Last n rows\n","\n","3. COLUMN OPERATIONS:\n","   df.columns                  # Column names\n","   df.dtypes                   # Data types\n","   df[['col1', 'col2']]       # Select columns\n","   df.drop(['col1'], axis=1)   # Remove columns\n","\n","4. ROW OPERATIONS:\n","   df[df['col'] > value]       # Filter rows\n","   df.sample(n=10)            # Random sample\n","   df.drop_duplicates()        # Remove duplicates\n","   df.dropna()                # Remove missing values\n","\n","5. AGGREGATIONS:\n","   df['col'].sum()            # Sum\n","   df['col'].mean()           # Average\n","   df.groupby('col').sum()    # Group by aggregation\n","   df.pivot_table()           # Pivot table\n","\n","6. EXPORTS:\n","   df.to_csv('output.csv', index=False)\n","   df.to_excel('output.xlsx', index=False)\n","   df.to_json('output.json', orient='records')\n","\"\"\"\n","\n","print(quick_ref)\n","\n","print(\"\\nüéâ CSV Analysis Tutorial Complete!\")\n","print(\"üí° Pro tip: Always start with df.info() and df.describe() to understand your data!\")"]}]}